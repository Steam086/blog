
	`class Worker(LocalOrDistributedWorkerBase):`，从继承关系可知，worker是分布式的开始
worker调用model_runner（a worker is associated with a single gpu）
model_runner负责：
- load_model
- execute_model
model_runner中的load_model方法调用了model_loader中的入口函数`get_model()`
```Python 
with DeviceMemoryProfiler() as m:
	self.model = get_model(vllm_config=self.vllm_config)
```
executor调用了worker，
```Python
# ExecutorBase中的方法：
def apply_model(self, func: Callable[[nn.Module], _R]) -> list[_R]:
# Run a function directly on the model inside each worker,
# returning the result for each of them.
	def rpc_func(worker: WorkerBase) -> _R:
		return func(worker.get_model())
	return self.collective_rpc(rpc_func)
```
entrypoints/llm.py中，分别包装了`executor`中的`apply_model`和`collective_rpc`
LLM类负责实现openai的接口


## vllm中的概念

- worker
A worker class that executes (a partition of) the model on a GPU.
Each worker is associated with a single GPU. The worker is responsible for maintaining the KV cache and executing the model on the GPU. In case of distributed inference, each worker is assigned a partition of the model.
- model_runner
GPU model runner with sampling step.
- executor
- model_loader
下载模型、加载模型？？
## 运行一个在Hugging Face上下载的模型

### vllm如何确定执行模型对应的代码
Hugging Face上的模型仓库通常是没有对应的代码文件的。
- Hugging Face上的模型都有一个config.json文件，其中第一个属性就是`architectures:[]`，vllm从Hugging Face上下载完成模型之后，通过`architecture`属性确定使用哪个python文件中的类执行这个模型。
- `vllm/model_executor/models`文件夹下有支持的多种模型，模型与对应推理代码的映射关系保存在`vllm/model_executor/models/registry.py`中。
比如，llama模型config.json的`architectures`属性为`["LlamaForCausalLM"]`，registry.py文件中的内容为：
```Python
models = {
"LlamaForCausalLM": ("llama", "LlamaForCausalLM"),
"$arch": ("$filename", "class name")
......
}
```
其中dict的key对应architectures，value是一个tuple，第一个元素是`vllm/model_executor/models`中的python文件名，第二个是.py文件中的类名。这样一来，模型提供了arch信息，vllm就能根据这个arch信息来动态执行对应的推理代码了。
## 什么是driver_worker
是一个tp_group中分发准备好的input的worker
```
tp_driver_worker is the list of workers that are rank 0 of each TP group EXCEPT global rank 0. These are the workers that will broadcast to the rest of the workers.
```

`This signals that there's no more requests to process for now. All workers are running infinite loop with broadcast_tensor_dict, and it stops the loop when the driver broadcasts an empty input. Send an empty input to notify all other workers to stop their execution loop.
`


worker: 
- 接收和发送pipeline的输出
- driver_worker预处理inputs并broadcast_tensor_dict()广播给其他非driver_worker

model_runner： 
- 接收或发送KV_cache
- 如果is_driver_worker而且是最后一个pp_rank，gather并计算logits，Sample the next token并返回